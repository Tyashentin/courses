{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import random\n",
    "\n",
    "random.seed(137)\n",
    "rest = random.random()\n",
    "\n",
    "def weight(word):\n",
    "    # overfitted\n",
    "    if word == 'lerxst@wam.umd.edu':\n",
    "        return 100.0\n",
    "    if word == 'car':\n",
    "        return random.random()\n",
    "    if word == 'dog':\n",
    "        return - random.random()\n",
    "    return random.random()\n",
    "\n",
    "def has(word, text):\n",
    "    return word in text \n",
    "\n",
    "def feature(index):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applied Machine Learning\n",
    "\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- We have some dataset\n",
    "- We identify the problem and define the loss function\n",
    "- Then we minimize the total loss (empirical risk, or objective) using available (training) data\n",
    "- We vary parameters to minimize the objective function\n",
    "- The minimizing parameters are then used to predict unknown values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A text classification problem\n",
    "\n",
    "Lets consider the **20 newsgroups** dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.autos\n",
      "----\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "text, label = data['data'][0], data['target_names'][data['target'][0]]\n",
    "print(label)\n",
    "print('----')\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A linear model for classification\n",
    "\n",
    "Let us consider a function that tells if the `text` comes from `rec.autos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20861117671669727"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight('car')*has('car', text) + weight('dog')*has('dog', text) + rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively say `car` is `0` and `dog` is `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6520663252475314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight(0)*feature(0) + weight(1)*feature(1) + rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How do we find those `weight` ($w$) for all the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- Last time we used `opt.fmin` and it magically found the solution\n",
    "- The method is simple though\n",
    "- Start with random weights $w_0$\n",
    "- Iterate: $w_{i+1} = w_{i} - \\alpha \\times \\nabla \\mathsf{objective}(w_i)$\n",
    "- All we need to know is the gradient of objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient of loss\n",
    "\n",
    "- Last time we considered a regression problem and used $(y-p)^2$\n",
    "- The gradient w.r.t $p$ is obvious: $- 2 (y - p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient check\n",
    "\n",
    "How can we ensure the gradient is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.39999999999999997, -0.400000000000001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return (y-p)**2\n",
    "\n",
    "def gradient(y, p):\n",
    "    return -2*(y-p)\n",
    "\n",
    "p = 0.1\n",
    "y = 0.3\n",
    "eps = 0.001\n",
    "gradient(y, p), (loss(y, p+eps) - loss(y, p-eps)) / (2*eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4584193179180418\n",
      "1 0.15742261387376238\n",
      "2 0.41477479583162125\n",
      "3 0.21796471467934872\n",
      "4 0.3516012199077561\n",
      "5 0.27174513927502175\n",
      "6 0.3132849772543624\n",
      "7 0.2947301870654056\n",
      "8 0.30172239020516817\n",
      "9 0.2995513193849228\n"
     ]
    }
   ],
   "source": [
    "current_p = random.random()\n",
    "alpha = 1.0\n",
    "\n",
    "for i in range(10):\n",
    "    current_p = current_p - alpha*(0.95)**i*gradient(y, current_p)\n",
    "    print(i, current_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_p = random.random()\n",
    "alpha = 0.1\n",
    "\n",
    "xs = list(range(20))\n",
    "ys = []\n",
    "for _ in xs:\n",
    "    current_p = current_p - alpha*gradient(y, current_p)\n",
    "    ys.append(current_p)\n",
    "    \n",
    "plt.plot(xs, ys); plt.hlines(y, xs[0], xs[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification loss\n",
    "\n",
    "- We will use something called **logistic loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 144.26950408889635)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return np.log2(1.0 + np.exp(-y*p))\n",
    "    \n",
    "loss(-1, -100.0), loss(-1, +100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.67918581, -9.78951397]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.linear_model.SGDClassifier(loss='log', tol=1e-6)\n",
    "example_1 = [1,0]; label_1 = [1]\n",
    "example_2 = [0,1]; label_2 = [0]\n",
    "model.fit([example_1, example_2], np.ravel([label_1, label_2]))\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "- We can always come up with a model that fits data perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight('lerxst@wam.umd.edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For some reason that's not what we want. Why?\n",
    "- First, we need to measure if such a thing happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Splitting the data\n",
    "\n",
    "- Obviously we should not test what we fit against\n",
    "- We should fit (train) the model on some part of data\n",
    "- Next, we check the model against the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave-on-out\n",
    "\n",
    "- Generate as many samples as there are examples\n",
    "- Gives you a good estimate if you don't have a lot of data\n",
    "- Gets impractical on huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] [0]\n",
      "[0 2 3 4] [1]\n",
      "[0 1 3 4] [2]\n",
      "[0 1 2 4] [3]\n",
      "[0 1 2 3] [4]\n"
     ]
    }
   ],
   "source": [
    "loo = sklearn.model_selection.LeaveOneOut()\n",
    "for train, test in loo.split([1,2,3,4,5]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation\n",
    "\n",
    "- Split the dataset into a few (say 5) non-overlapping parts\n",
    "- Four parts go to training data and one part goes to test data\n",
    "- Do the above 5 times to train the model and test it\n",
    "- Makes a decent way to *detect* overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation in sklearn\n",
    "\n",
    "Let's consider indices of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5] [0 1]\n",
      "[0 1 4 5] [2 3]\n",
      "[0 1 2 3] [4 5]\n"
     ]
    }
   ],
   "source": [
    "xval = sklearn.model_selection.KFold(n_splits=3)\n",
    "for train, test in xval.split([1,2,3,4,5,6]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This thing is an ill-posed problem\n",
    "\n",
    "- A mathematical problem is ill-posed when the solution is not unique\n",
    "- That's exactly the case of regression/classification/...\n",
    "- We need to make the problem well-posed: *regularization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structural risk minimization\n",
    "\n",
    "- Structural risk is empirical risk plus regularizer\n",
    "- Instead of minimizing empirical risk we find some tradeoff\n",
    "- Regularizer is a function of model we get\n",
    "- $\\mathsf{objective} = \\mathsf{loss} + \\mathsf{regularizer}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularizer\n",
    "\n",
    "- A functions that reflects the complexity of a model\n",
    "- What is the complexity of a set of 'if ... then'?\n",
    "- Not obvious for linear model but easy to invent something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_1$ regularizer\n",
    "\n",
    "- Derivative is const\n",
    "- Forces weight to be zero if it doesn't hurt performance much \n",
    "- Use if you believe some features are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.SGDClassifier(loss='log', penalty='l1');\n",
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='l1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_2$ regularizer\n",
    "\n",
    "- Derivative is linear\n",
    "- Forces weights to get *similar* magnitude if it doesn't hurt performance much\n",
    "- Use if you believe all features are more or less important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.SGDClassifier(loss='log', penalty='l2');\n",
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='l2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic net\n",
    "\n",
    "- Just a weighted sum of $\\ell_1$ and $\\ell_2$ regularizers\n",
    "- An attempt to get useful properties of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of linearity\n",
    "\n",
    "- In low-dimensional spaces linear models are not very 'powerful' (can we define that?)\n",
    "- The higher dimensionality, the more powerful linear model becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse features\n",
    "\n",
    "- We say features are sparse when most of the values are zero\n",
    "- Examples: visited hosts, movies that user liked, ...\n",
    "- Sparse features are efficient in high-dimensional setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0, 0, ..., 1, ..., 0, 0, 1, 0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One hot encoding, hashing trick\n",
    "\n",
    "- One way to encode categorical things like visited websites\n",
    "- We enumerate all the websites\n",
    "- We put 1 to position of every host, 0 otherwise\n",
    "- Hashing trick: instead of enumerating them just hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4630140204804810619\n",
      "55429\n"
     ]
    }
   ],
   "source": [
    "print(hash('hse.ru'))\n",
    "print(hash('hse.ru') % 2**16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hashing vectorizer in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.70710678 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.70710678 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=10, binary=True)\n",
    "features = vectorizer.fit_transform(['hello there', 'hey there'])\n",
    "print(features.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When do we use linear models?\n",
    "\n",
    "- It is definitely the first thing to try if you have some text data\n",
    "- In general a good choice for any sparse data\n",
    "- This approach is pretty much the fastest one\n",
    "- Even if some method outperforms, you still get a good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-assesment questions\n",
    "\n",
    "1. You noticed that your linear model learned a weight of **95.3** for the word `the`. *Is there a problem? Y/N*\n",
    "2. The train loss is **0.43** and the test loss is **0.39**. *Is it an example of ..? a) overfitting b) underfitting c) I don't know*\n",
    "3. You've got basically infinite amounts of data. *Do you have to use regularization? Y/N*\n",
    "4. You believe your dataset is pretty noisy and some features are broken. *You use a) L1 b) L2 c) no regularization* \n",
    "5. Why do we hash words? *a) it's simpler b) it's faster c) it's more reliable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Homework 1\n",
    "\n",
    "- No score, just has to be done\n",
    "- Load dataset, create linear model, train, and explain the results\n",
    "- The template is provided: `HSE-AML-HW1.ipynb`\n",
    "- Hint: check the code examples for `KFold`, `HashingVectorizer`, `LogisticRegression`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
